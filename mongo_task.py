
from datetime import datetime, timedelta, timezone
from pymongo import MongoClient
import json
import os


# –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
MONGO_URI = os.environ.get("MONGO_URI", "mongodb://localhost:27017/")
DB_NAME = os.environ.get("MONGO_DB", "my_database")
SRC_COLL = "user_events"
ARCHIVE_COLL = "archived_users"


# –ø–æ—Ä–æ–≥–æ–≤—ã–µ –¥–∞—Ç—ã 
now = datetime.now(timezone.utc)   
registered_before = now - timedelta(days=30)
inactive_since = now - timedelta(days=14)


def main():
    client = MongoClient(MONGO_URI)
    db = client[DB_NAME]
    events = db[SRC_COLL]
    archived = db[ARCHIVE_COLL]

    # –ø–æ—Å–ª–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –¥–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
    pipeline = [
        {
            "$group": {
                "_id": "$user_id",
                "last_activity": {"$max": "$event_time"},
                "registration_date": {"$min": "$user_info.registration_date"},
            }
        },
        {
            # —Ç–æ–ª—å–∫–æ —Ç–µ, –∫—Ç–æ —Å—Ç–∞—Ä—à–µ 30 –¥–Ω–µ–π –∏ –Ω–µ –∞–∫—Ç–∏–≤–µ–Ω 14 –¥–Ω–µ–π
            "$match": {
                "$expr": {
                    "$and": [
                        {"$lte": ["$registration_date", registered_before]},
                        {"$lt": ["$last_activity", inactive_since]},
                    ]
                }
            }
        },
    ]

    candidates = list(events.aggregate(pipeline))

    archived_user_ids = []
    archived_at = now

    for doc in candidates:
        user_id = doc["_id"]
        reg = doc.get("registration_date")
        last_act = doc.get("last_activity")

        # upsert: —Å–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç; –µ—Å–ª–∏ –∑–∞–ø–∏—Å—å –µ—Å—Ç—å, —Ç–æ –Ω–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º archived_at
        archived.update_one(
            {"user_id": user_id},
            {
                "$setOnInsert": {
                    "user_id": user_id,
                    "registration_date": reg,
                    "last_activity": last_act,
                    "archived_at": archived_at,
                    "reason": {
                        "registered_before": registered_before,
                        "inactive_since": inactive_since,
                    },
                }
            },
            upsert=True,
        )

        # –ø—Ä–æ–≤–µ—Ä–∫–∞: –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–ø–µ—Ä–≤—ã–µ –ø–æ–ø–∞–ª –≤ –∞—Ä—Ö–∏–≤ —Å–µ–≥–æ–¥–Ω—è,
        # –∏–Ω–∞—á–µ - –≤ —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–π –æ—Ç—á—ë—Ç –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º
        was_inserted = db.command(
            "find", ARCHIVE_COLL,
            filter={"user_id": user_id, "archived_at": archived_at}
        )["cursor"]["firstBatch"]
        if was_inserted:
            archived_user_ids.append(user_id)

    # –æ—Ç—á—ë—Ç
    report_date_str = now.date().isoformat()  # YYYY-MM-DD
    report_dir = os.path.join(os.getcwd(), "reports")
    os.makedirs(report_dir, exist_ok=True)
    report_path = os.path.join(report_dir, f"{report_date_str}.json")

    report = {
        "date": report_date_str,
        "archived_users_count": len(archived_user_ids),
        "archived_user_ids": sorted(archived_user_ids),
    }

    # json
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=4)

    print(f"‚úÖ –ê—Ä—Ö–∏–≤–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–æ–±–∞–≤–ª–µ–Ω–æ: {len(archived_user_ids)}")
    print(f"üßæ –û—Ç—á—ë—Ç: {report_path}")

if __name__ == "__main__":
    main()

##################################################################################################